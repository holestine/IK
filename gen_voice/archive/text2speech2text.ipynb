{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\holes\\.conda\\envs\\ik\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Standard Imports\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Utility for collecting execution times\n",
    "from timer import Timer\n",
    "\n",
    "# Audio libraries\n",
    "from openai import OpenAI\n",
    "import speech_recognition as sr\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from gtts import gTTS\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "from transformers import pipeline\n",
    "\n",
    "# LLM\n",
    "from chatgpt import get_completion\n",
    "\n",
    "# Personal access tokens\n",
    "from key import openai_key, wit_key\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('./waves', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "gTTS ran 3 times\n",
      "\tMin time was 410.99071502685547 at index 2\n",
      "\tMax time was 571.526288986206 at index 1\n",
      "\tAverage time was 492.14943250020343 ms\n",
      "\tTotal time was 1476.4482975006104 ms\n",
      "\n",
      "alloy ran 3 times\n",
      "\tMin time was 1183.842420578003 at index 0\n",
      "\tMax time was 1483.2725524902344 at index 2\n",
      "\tAverage time was 1311.5046819051106 ms\n",
      "\tTotal time was 3934.514045715332 ms\n",
      "\n",
      "echo ran 3 times\n",
      "\tMin time was 837.2936248779297 at index 0\n",
      "\tMax time was 1292.8078174591064 at index 1\n",
      "\tAverage time was 1045.8238919576008 ms\n",
      "\tTotal time was 3137.4716758728027 ms\n",
      "\n",
      "fable ran 3 times\n",
      "\tMin time was 978.9559841156006 at index 1\n",
      "\tMax time was 1525.5086421966553 at index 2\n",
      "\tAverage time was 1275.2206325531006 ms\n",
      "\tTotal time was 3825.6618976593018 ms\n",
      "\n",
      "onyx ran 3 times\n",
      "\tMin time was 711.677074432373 at index 1\n",
      "\tMax time was 1143.1801319122314 at index 2\n",
      "\tAverage time was 920.8787282307943 ms\n",
      "\tTotal time was 2762.636184692383 ms\n",
      "\n",
      "nova ran 3 times\n",
      "\tMin time was 851.9587516784668 at index 0\n",
      "\tMax time was 2004.4822692871094 at index 2\n",
      "\tAverage time was 1384.872277577718 ms\n",
      "\tTotal time was 4154.616832733154 ms\n",
      "\n",
      "shimmer ran 3 times\n",
      "\tMin time was 813.8210773468018 at index 1\n",
      "\tMax time was 1410.4926586151123 at index 2\n",
      "\tAverage time was 1077.469825744629 ms\n",
      "\tTotal time was 3232.4094772338867 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an OpenAI client\n",
    "client = OpenAI(\n",
    "    api_key=openai_key,\n",
    ")\n",
    "\n",
    "# The different voices supported by OpenAI\n",
    "voices = ['alloy', 'echo', 'fable', 'onyx', 'nova', 'shimmer']\n",
    "#voices = []\n",
    "\n",
    "# A few test phrases\n",
    "phrases = [\"Suzy sells sea shells by the sea shore\", \n",
    "           \"Peter Piper picked a peck of pickled peppers\",\n",
    "           \"The longest word in the English language that doesn't repeat a character is uncopyrightable\"]\n",
    "\n",
    "# To keep track of the generated files\n",
    "voice_paths = []\n",
    "\n",
    "# Generate a wav file for each phrase and voice\n",
    "for i, phrase in enumerate(phrases):\n",
    "    voice_paths.append(f'./waves/gTTS{i}_speech.wav')\n",
    "    \n",
    "    with Timer('gTTS'):\n",
    "        gTTS(phrase).save(voice_paths[-1])\n",
    "        # gTTS saves in mp3 format, need to convert to wav\n",
    "        audio = AudioSegment.from_mp3(voice_paths[-1])\n",
    "        audio.export(voice_paths[-1], format=\"wav\")\n",
    "\n",
    "    for voice in voices:\n",
    "        #time.sleep(20) # Sleep so we don't hit the OpenAI RPM while gathering metrics\n",
    "        voice_paths.append(f'./waves/{voice}{i}_speech.wav')\n",
    "\n",
    "        with Timer(voice):\n",
    "            with client.audio.speech.with_streaming_response.create(\n",
    "                model=\"tts-1\",\n",
    "                voice=voice,\n",
    "                input=phrase\n",
    "            ) as response:\n",
    "                                \n",
    "                response.stream_to_file(voice_paths[-1])\n",
    "\n",
    "# Print execution times\n",
    "Timer().report()\n",
    "Timer().reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes audio files don't have the necessary headers, this will add them\n",
    "for voice_path in voice_paths:\n",
    "    x,_ = librosa.load(voice_path, sr=16000)\n",
    "    sf.write(voice_path, x, 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize speech recognition object\n",
    "recognizer = sr.Recognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wav2text(voice_path, service):\n",
    "    '''\n",
    "    Method to convert speech to text\n",
    "    '''\n",
    "\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    transcriber = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base.en\", device=device)\n",
    "\n",
    "    with sr.AudioFile(voice_path) as source:\n",
    "        audio = recognizer.record(source)  # read the entire audio file\n",
    "    data, fs = sf.read(voice_path, dtype='float32')\n",
    "\n",
    "    with Timer(service):\n",
    "        if service == 'Sphinx':\n",
    "            text = recognizer.recognize_sphinx(audio)\n",
    "        elif service =='GoogleSpeech':\n",
    "            text = recognizer.recognize_google(audio)\n",
    "        elif service =='GoogleCloudSpeech':\n",
    "            # You will need a Google Cloud account, enable the Cloud Speech-to-Text API\n",
    "            # and create a credentials file to use this service. Get started by following\n",
    "            # these instructions https://cloud.google.com/iam/docs/keys-create-delete#python\n",
    "            text = recognizer.recognize_google_cloud(audio, 'google_cloud_credentials.json')\n",
    "        elif service =='Wit':\n",
    "            # You will need to create an account at https://wit.ai/ and generate \n",
    "            # a 'Client Access Token' for the key argument below.\n",
    "            text = recognizer.recognize_wit(audio, key=wit_key)\n",
    "        elif service == 'OpenAI':\n",
    "            audio_file= open(voice_path, \"rb\")\n",
    "            text = client.audio.transcriptions.create(model=\"whisper-1\",  file=audio_file).text\n",
    "        elif service == 'Whisper':\n",
    "            #text = transcriber({\"sampling_rate\": fs, \"raw\": data})[\"text\"]\n",
    "            text = transcriber(data)[\"text\"]\n",
    "            \n",
    "    try:\n",
    "        print(f\"{service} thinks you said: \\\"{text}\\\"\")\n",
    "    except sr.UnknownValueError:\n",
    "        print(f\"{service} could not understand audio\")\n",
    "    except sr.RequestError as e:\n",
    "        print(f\"{service} error; {0}\".format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: ./waves/gTTS0_speech.wav\n",
      "GoogleSpeech thinks you said: \"Susie sells seashells by the seashore\"\n",
      "Wit thinks you said: \"Suzy sells seashells by the seashore\"\n",
      "OpenAI thinks you said: \"Susie sells seashells by the seashore.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\holes\\.conda\\envs\\ik\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "c:\\Users\\holes\\.conda\\envs\\ik\\Lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:598: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper thinks you said: \" Susie sells seashells by the Seashore.\"\n",
      "\n",
      "Reading file: ./waves/gTTS1_speech.wav\n",
      "GoogleSpeech thinks you said: \"Peter Piper picked a peck of pickled peppers\"\n",
      "Wit thinks you said: \"Peter Piper picked a pec of pickled peppers\"\n",
      "OpenAI thinks you said: \"Peter Piper picked a peck of pickled peppers.\"\n",
      "Whisper thinks you said: \" Peter Piper picked a peck of pickled peppers.\"\n",
      "\n",
      "Reading file: ./waves/gTTS2_speech.wav\n",
      "GoogleSpeech thinks you said: \"the longest word in the English language that doesn't repeat a character is uncopyrightable\"\n",
      "Wit thinks you said: \"The longest word in the English language that doesn't repeat a character is on copyrightable\"\n",
      "OpenAI thinks you said: \"The longest word in the English language that doesn't repeat a character is uncopyrightable.\"\n",
      "Whisper thinks you said: \" The longest word in the English language that doesn't repeat a character is uncopuritable.\"\n",
      "\n",
      "\n",
      "\n",
      "GoogleSpeech ran 3 times\n",
      "\tMin time was 737.8661632537842 at index 1\n",
      "\tMax time was 1527.05717086792 at index 2\n",
      "\tAverage time was 1190.4945373535156 ms\n",
      "\tTotal time was 3571.483612060547 ms\n",
      "\n",
      "Wit ran 3 times\n",
      "\tMin time was 610.9151840209961 at index 1\n",
      "\tMax time was 830.9056758880615 at index 2\n",
      "\tAverage time was 692.0297145843506 ms\n",
      "\tTotal time was 2076.0891437530518 ms\n",
      "\n",
      "OpenAI ran 3 times\n",
      "\tMin time was 551.5761375427246 at index 1\n",
      "\tMax time was 646.8205451965332 at index 2\n",
      "\tAverage time was 594.8417981465658 ms\n",
      "\tTotal time was 1784.5253944396973 ms\n",
      "\n",
      "Whisper ran 3 times\n",
      "\tMin time was 146.62814140319824 at index 1\n",
      "\tMax time was 2601.8261909484863 at index 0\n",
      "\tAverage time was 985.6206576029459 ms\n",
      "\tTotal time was 2956.861972808838 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert all the voices back to text\n",
    "for voice_path in voice_paths:\n",
    "    #time.sleep(20) # Sleep so we don't hit the OpenAI RPM while gathering metrics\n",
    "        \n",
    "    print(f'Reading file: {voice_path}')\n",
    "    #wav2text(voice_path, 'Sphinx')               # Didn't work well on Linux and gives error on Windows\n",
    "    wav2text(voice_path, 'GoogleSpeech')\n",
    "    #wav2text(voice_path, 'GoogleCloudSpeech')    # Not one of the best contenders and gives error on Windows\n",
    "    wav2text(voice_path, 'Wit')\n",
    "    wav2text(voice_path, 'OpenAI')\n",
    "    wav2text(voice_path, 'Whisper')\n",
    "    print('')\n",
    "\n",
    "# Print execution times\n",
    "Timer().report()\n",
    "Timer().reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available microphones\n",
    "sr.Microphone.list_microphone_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize microphone object with appropriate device\n",
    "microphone = sr.Microphone(device_index=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_speech_from_mic(recognizer, microphone):\n",
    "    \"\"\" Transcribe speech from the microphone\n",
    "\n",
    "    Returns a dictionary with three keys:\n",
    "    \"success\": a boolean indicating whether or not the API request was successful\n",
    "    \"error\": `None` if no error occured, otherwise a string containing an error message if the API could not be reached or speech was unrecognizable\n",
    "    \"transcription\": A string containing the transcribed text or `None` if speech was unrecognizable\n",
    "    \"\"\"\n",
    "\n",
    "    # Check that recognizer and microphone arguments are appropriately typed\n",
    "    if not isinstance(recognizer, sr.Recognizer):\n",
    "        raise TypeError(\"`recognizer` must be `Recognizer` instance\")\n",
    "\n",
    "    if not isinstance(microphone, sr.Microphone):\n",
    "        raise TypeError(\"`microphone` must be `Microphone` instance\")\n",
    "\n",
    "    # Adjust the recognizer sensitivity for ambient noise and listen to the microphone\n",
    "    with microphone as source:\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        audio = recognizer.listen(source)\n",
    "\n",
    "    # Initialize response object\n",
    "    response = {\n",
    "        \"success\": True,\n",
    "        \"error\": None,\n",
    "        \"transcription\": None\n",
    "    }\n",
    "\n",
    "    # Try to recognize the speech if a RequestError or UnknownValueError exception is caught update the response object accordingly\n",
    "    try:\n",
    "        response[\"transcription\"] = recognizer.recognize_google(audio)\n",
    "    except sr.RequestError:\n",
    "        # API was unreachable or unresponsive\n",
    "        response[\"success\"] = False\n",
    "        response[\"error\"] = \"API unavailable\"\n",
    "    except sr.UnknownValueError:\n",
    "        # speech was unintelligible\n",
    "        response[\"error\"] = \"Unable to recognize speech\"\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def communicate(phrase):\n",
    "    temp_file = 'waves\\\\temp.mp3'\n",
    "    gTTS(phrase).save(temp_file)\n",
    "    audio = AudioSegment.from_mp3(temp_file)\n",
    "    play(audio)\n",
    "    os.remove(temp_file)\n",
    "\n",
    "while True:\n",
    "    guess = recognize_speech_from_mic(recognizer, microphone)['transcription']\n",
    "    if guess is None:\n",
    "        communicate(\"Sorry, I didn't understand that\")\n",
    "    elif 'hello' in guess:\n",
    "        communicate('Hello, how can I help')  \n",
    "    elif 'goodbye' in guess:\n",
    "        communicate('See you next time')\n",
    "        break\n",
    "    else:\n",
    "        print(f'Prompt: {guess} \\n')\n",
    "        response = get_completion(guess)\n",
    "        \n",
    "        print(f'{response}\\n\\n')\n",
    "        communicate(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ik",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
