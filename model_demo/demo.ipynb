{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from vit_pytorch import ViT\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "from timer import Timer\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLPNet(nn.Module):\n",
    "    def __init__(self, image_size, num_classes, channels):\n",
    "        super(MLPNet, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(image_size * channels, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, num_classes)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNNet(nn.Module):\n",
    "    def __init__(self, num_classes, channels, image_height, image_width):\n",
    "        super(CNNNet, self).__init__()\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "        self.norm1 = nn.BatchNorm2d(32)\n",
    "        self.norm2 = nn.BatchNorm2d(64)\n",
    "        self.norm3 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(channels, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, 1)\n",
    "\n",
    "        fc_size = self.forward(torch.rand((1, channels, image_height, image_width)), True)\n",
    "\n",
    "        self.fc1 = nn.Linear(fc_size, 64)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x, get_fc_size=False):\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv3(x)\n",
    "        x = self.norm3(x)\n",
    "        x = F.relu(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        if get_fc_size:\n",
    "            return x.nelement()\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTNet(nn.Module): \n",
    "    def __init__(self, image_size, patch_size, num_classes, channels):\n",
    "        HIDDEN_SIZE = 768\n",
    "        DEPTH = 6\n",
    "        HEADS = 12\n",
    "        MLP_DIM = 3072\n",
    "        \n",
    "        super(ViTNet, self).__init__()\n",
    "        \n",
    "        self.model = ViT(\n",
    "                        image_size = image_size,\n",
    "                        patch_size = patch_size,\n",
    "                        num_classes = num_classes,\n",
    "                        dim = HIDDEN_SIZE,\n",
    "                        depth = DEPTH,\n",
    "                        heads = HEADS,\n",
    "                        mlp_dim = MLP_DIM,\n",
    "                        channels = channels,\n",
    "                        dropout = 0.1,\n",
    "                        emb_dropout = 0.1\n",
    "                        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridNet(nn.Module):\n",
    "    def __init__(self, image_size, channels, num_classes):\n",
    "        HIDDEN_SIZE = 768\n",
    "        DEPTH = 6\n",
    "        HEADS = 12\n",
    "        MLP_DIM = 3072\n",
    "\n",
    "        super(HybridNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(channels, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "\n",
    "        self.norm1 = nn.BatchNorm2d(32)\n",
    "        self.norm2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        image_width = int((math.sqrt(image_size) - 4) / 2)\n",
    "\n",
    "        self.vit = ViT(\n",
    "                        image_size = image_width ** 2,\n",
    "                        patch_size = int(image_width / 2),\n",
    "                        num_classes = num_classes,\n",
    "                        dim = HIDDEN_SIZE,\n",
    "                        depth = DEPTH,\n",
    "                        heads = HEADS,\n",
    "                        mlp_dim = MLP_DIM,\n",
    "                        channels = 64,\n",
    "                        dropout = 0.1,\n",
    "                        emb_dropout = 0.1\n",
    "                        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.vit(x)\n",
    "\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, dataset, modeltype, writer, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            with Timer(\"{} {}\".format(modeltype, dataset)):\n",
    "                output = model(data)\n",
    "\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        accuracy))\n",
    "    writer.add_scalar('Accuracy/{} {}'.format(modeltype, dataset), accuracy, epoch)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(batch_size, test_batch_size, lr, gamma, epochs, log_dir, log_interval):\n",
    "    # Determine if we should use a GPU\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "\n",
    "    # Reduce precision to speed up training\n",
    "    torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "    if use_cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    train_kwargs = {'batch_size': batch_size}\n",
    "    test_kwargs  = {'batch_size': test_batch_size}\n",
    "    if use_cuda:\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    accuracies = {}\n",
    "    parameters = {}\n",
    "    \n",
    "    for dataset in DATASETS:\n",
    "        if dataset == \"MNIST\":\n",
    "            transform=transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.1307), (0.3081))\n",
    "            ])\n",
    "            train_set = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "            test_set = datasets.MNIST('./data', train=False, transform=transform)\n",
    "            image_height = 28\n",
    "            image_width = 28\n",
    "            patch_size = 14\n",
    "            num_classes = 10\n",
    "            channels = 1\n",
    "        elif dataset == \"CIFAR10\":\n",
    "            transform=transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "            ])\n",
    "            train_set = datasets.CIFAR10('./data', train=True, download=True, transform=transform)\n",
    "            test_set = datasets.CIFAR10('./data', train=False, transform=transform)\n",
    "            image_height = 32\n",
    "            image_width = 32\n",
    "            patch_size = 16\n",
    "            num_classes = 10\n",
    "            channels = 3\n",
    "        elif dataset == \"CIFAR100\":\n",
    "            transform=transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "            ])\n",
    "            train_set = datasets.CIFAR100('./data', train=True, download=True, transform=transform)\n",
    "            test_set = datasets.CIFAR100('./data', train=False, transform=transform)\n",
    "            image_height = 32\n",
    "            image_width = 32\n",
    "            patch_size = 16\n",
    "            num_classes = 100\n",
    "            channels = 3\n",
    "\n",
    "        for model_type in MODEL_TYPES:\n",
    "            print(\"Evaluating {} with {}\".format(model_type, dataset))\n",
    "\n",
    "            train_loader = torch.utils.data.DataLoader(train_set,**train_kwargs)\n",
    "            test_loader  = torch.utils.data.DataLoader(test_set, **test_kwargs)\n",
    "\n",
    "            if model_type == \"MLP\":\n",
    "                model = MLPNet(image_height * image_width, num_classes, channels).to(device)\n",
    "            elif model_type == \"CNN\":\n",
    "                model = CNNNet(num_classes, channels, image_height, image_width).to(device)\n",
    "            elif model_type == \"ViT\":\n",
    "                model = ViTNet(image_height * image_width, patch_size, num_classes, channels).to(device)\n",
    "            elif model_type == \"HYBRID\":\n",
    "                model = HybridNet(image_height * image_width, channels, num_classes).to(device)\n",
    "            \n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "            scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "\n",
    "            max_accuracy = 0\n",
    "            writer = SummaryWriter(log_dir=log_dir)\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                train(model, device, train_loader, optimizer, epoch, log_interval)\n",
    "                accuracy = test(model, device, test_loader, dataset, model_type, writer, epoch)\n",
    "                max_accuracy = max(accuracy, max_accuracy)\n",
    "                scheduler.step()\n",
    "                if (max_accuracy > 98):\n",
    "                    print('Breaking at epoch {}'.format(epoch))\n",
    "                    break\n",
    "\n",
    "            accuracies[\"{} {}\".format(model_type, dataset)] = max_accuracy\n",
    "            parameters[\"{} {}\".format(model_type, dataset)] = count_parameters(model)\n",
    "\n",
    "            torch.save(model.state_dict(), \"{}_{}.pt\".format(model_type, dataset))\n",
    "\n",
    "    return accuracies, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating MLP with MNIST\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.303497\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.292361\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.319742\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.313024\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.309254\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.309837\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.301469\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.311362\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.304074\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.301981\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.285064\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.295862\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.301253\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 2.289732\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.292773\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.294828\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 2.287679\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 2.271045\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 2.287699\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 2.282028\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.268780\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 2.259457\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 2.252626\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 2.272497\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 2.273533\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 2.250853\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 2.254548\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 2.259341\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 2.242614\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 2.270926\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.250490\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 2.210120\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 2.236749\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 2.233058\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 2.232583\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 2.185973\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 2.216393\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 2.201411\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 2.180048\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 2.189222\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.204861\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 2.176893\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 2.186024\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 2.169439\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 2.151924\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 2.149671\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 2.167543\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 2.151885\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 2.110982\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 2.124548\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.153557\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 2.117288\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 2.065593\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 2.119647\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 2.123846\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 2.106129\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 2.100968\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 2.087314\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 2.008797\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 1.988373\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.041425\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 2.042247\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 1.993289\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 1.934182\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 1.927184\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 2.059609\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 1.981071\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 1.854580\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 1.994231\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 1.931731\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 1.884744\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 1.911942\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 1.855590\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 1.815186\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 1.811011\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 1.799472\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 1.835996\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 1.851392\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 1.859953\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 1.809018\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.759684\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 1.926060\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 1.707145\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 1.750361\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 1.680043\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 1.712067\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 1.877240\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 1.672298\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 1.644400\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 1.694868\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.629141\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 1.597914\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 1.688090\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 1.759681\n",
      "\n",
      "Test set: Average loss: 1.4541, Accuracy: 6543/10000 (65%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.456501\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 1.689781\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 1.521874\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 1.504135\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 1.644614\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 1.587148\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 1.484312\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 1.506082\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 1.595303\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 1.552672\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 1.469062\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 1.567730\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 1.548568\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 1.442755\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 1.456794\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 1.419580\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 1.514258\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 1.405160\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 1.422388\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 1.445364\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 1.370500\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 1.509542\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 1.367301\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 1.373168\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 1.474854\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 1.397899\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 1.373390\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 1.334091\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 1.441895\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 1.352399\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 1.397371\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 1.347111\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 1.297050\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 1.315623\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 1.331045\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 1.422445\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 1.307885\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 1.304330\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 1.313662\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 1.290046\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 1.197422\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 1.279367\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 1.224669\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 1.075728\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 1.166904\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 1.168968\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 1.334345\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 1.325538\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 1.284142\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 1.258461\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 1.230441\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 1.349983\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 1.167993\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 1.322306\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 1.191832\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 1.109798\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 1.254363\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 1.311823\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 1.106037\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 1.286242\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 1.136172\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 1.216254\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 1.276564\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 1.124151\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 1.168787\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 1.221236\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 1.141615\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 1.353176\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 1.187286\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 1.079731\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 1.145689\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 1.273200\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 1.038442\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 1.028764\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 1.257027\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 1.137331\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 1.164886\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 1.100159\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 1.072623\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 1.129212\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.029469\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 1.064206\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 1.073114\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 1.044024\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 1.114306\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 1.158106\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 1.060909\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 1.115842\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 1.000883\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 1.098386\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 1.090085\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 1.170607\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 1.100035\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 1.004449\n",
      "\n",
      "Test set: Average loss: 0.8689, Accuracy: 7988/10000 (80%)\n",
      "\n",
      "Evaluating CNN with MNIST\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.321516\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.311748\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.291650\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.287654\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.291536\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.296179\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.293549\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.275645\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.284074\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.249664\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.257177\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.253917\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.229359\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 2.245739\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.228777\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.224172\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 2.190563\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 2.195411\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 2.204163\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 2.178600\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.152013\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 2.167498\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 2.124186\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 2.136898\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 2.115566\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 2.132844\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 2.118550\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 2.078101\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 2.075084\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 2.038466\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.074847\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 1.938174\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 2.071415\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 2.016355\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 2.017206\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 1.888086\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 1.998242\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 1.909203\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 1.839385\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 1.876026\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.865366\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 1.784858\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 1.735634\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 1.707085\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 1.787298\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 1.782672\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 1.770346\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 1.652110\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 1.677805\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 1.723362\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.603147\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 1.568991\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 1.554537\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 1.550688\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 1.640971\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 1.513290\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 1.526605\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 1.359579\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 1.424197\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 1.454369\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.334087\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 1.324830\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 1.336975\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 1.256602\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 1.364301\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 1.226019\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 1.226121\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 1.359229\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 1.093950\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 1.101697\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 1.168070\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 1.127138\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 1.091549\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 1.062687\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 1.003254\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 1.004395\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 1.046247\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.990698\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.823264\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 1.161317\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.910738\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.971466\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.910627\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.875097\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.857824\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.924896\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.839477\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.754598\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.850676\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.828211\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.856986\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.872533\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.846545\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.724292\n",
      "\n",
      "Test set: Average loss: 0.8888, Accuracy: 8466/10000 (85%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.904357\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.734557\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.791727\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.932518\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.618137\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.646318\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.723519\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.854941\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.768257\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.773357\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.792306\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.702849\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.655697\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.588466\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.540145\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.613286\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.753996\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.737582\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.744560\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.553307\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.658211\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.592271\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.720043\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.572019\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.613368\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.551326\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.570553\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.766089\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.609472\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.498887\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.581139\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.701674\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.649509\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.529254\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.640590\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.745735\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.627902\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.602507\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.748721\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.616648\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.562493\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.563843\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.691197\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.583054\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.490692\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.451908\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.531673\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.736681\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.548368\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.536699\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.772991\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.623645\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.501657\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.482211\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.624185\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.468556\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.412749\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.564735\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.689962\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.525161\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.435758\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.506384\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.479824\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.445938\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.565325\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.407748\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.861776\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.592418\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.395762\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.450053\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.388447\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.483853\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.529168\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.603024\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.450169\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.376833\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.665372\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.446768\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.522204\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.422474\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.573816\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.548471\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.329322\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.552546\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.532784\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.397145\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.321220\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.513228\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.556884\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.658596\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.393074\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.452925\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.457252\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.486139\n",
      "\n",
      "Test set: Average loss: 0.5140, Accuracy: 8850/10000 (88%)\n",
      "\n",
      "Evaluating ViT with MNIST\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.435216\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.184247\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.021828\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 1.821865\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 1.680191\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 1.513758\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 1.052241\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 1.204962\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.806847\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.793736\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.752610\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.741292\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.465357\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.441002\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.370745\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.437117\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.431447\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.423321\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.355435\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.347318\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.516741\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.341527\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.533200\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.213168\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.427866\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.656011\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.449087\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.494085\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.345135\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.260854\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.321231\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.256335\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.470828\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.203712\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.369464\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.276073\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.374915\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.252068\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.178235\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.574355\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.295152\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.377892\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.297631\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.305682\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.139898\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.430110\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.424413\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.192982\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.466960\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.234695\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.228701\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.324523\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.339662\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.105299\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.205025\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.182538\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.397974\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.392117\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.330444\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.231101\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.346506\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.371305\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.533145\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.299620\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.496245\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.298046\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.324110\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.223690\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.223483\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.287887\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.238723\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.153716\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.232301\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.337672\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.301425\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.339436\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.100645\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.335907\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.472927\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.214827\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.209143\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.330485\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.352358\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.323940\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.229179\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.377168\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.212076\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.436589\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.210397\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.136469\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.148133\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.377132\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.232516\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.188112\n",
      "\n",
      "Test set: Average loss: 0.1823, Accuracy: 9481/10000 (95%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.098666\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.201031\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.156138\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.146798\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.090038\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.282649\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.115958\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.132042\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.053989\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.128766\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.150587\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.163433\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.248023\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.254557\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.121199\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.206060\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.327687\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.230803\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.077448\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.061337\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.381271\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.091968\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.162236\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.149082\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.190111\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.118186\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.113269\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.123073\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.148721\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.205971\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.257033\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.176634\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.149693\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.329746\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.207408\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.137482\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.142065\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.056815\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.177267\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.207908\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.128171\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.174948\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.205454\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.083715\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.244125\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.221541\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.220556\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.122193\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.102927\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.216641\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.140371\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.136545\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.249984\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.262684\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.291370\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.175482\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.227523\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.316401\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.283125\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.108207\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.327398\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.126772\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.149147\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.143187\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.200108\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.124349\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.187224\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.154702\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.175441\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.123386\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.116830\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.085948\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.305080\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.215915\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.161029\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.228460\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.163672\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.122561\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.134213\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.166783\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.202448\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.294177\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.088521\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.145524\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.053422\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.149880\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.223071\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.168133\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.187796\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.155521\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.083788\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.130764\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.068919\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.059828\n",
      "\n",
      "Test set: Average loss: 0.1196, Accuracy: 9644/10000 (96%)\n",
      "\n",
      "Evaluating HYBRID with MNIST\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.349806\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.179299\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.015914\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 1.941994\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 1.491057\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 1.383627\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 1.166653\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.976940\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.663507\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.551073\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.579977\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.567041\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.482112\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.333098\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.292955\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.255376\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.217907\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.379870\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.309666\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.363731\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.329758\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.270362\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.183392\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.333583\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.402608\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.290014\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.258539\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.145286\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.225527\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.236315\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.287292\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.096862\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.251370\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.174056\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.436748\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.272001\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.244647\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.201692\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.125517\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.205672\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.488979\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.287305\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.095372\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.199115\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.399285\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.174352\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.236396\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.218155\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.247304\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.148130\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.210003\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.245655\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.213066\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.143919\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.108603\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.235766\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.140137\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.116865\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.194320\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.132001\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.104583\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.095356\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.051659\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.073224\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.353231\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.086530\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.261675\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.204748\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.097942\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.039000\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.042065\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.139229\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.097314\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.062395\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.111724\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.170742\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.053764\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.091318\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.319832\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.069663\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.101444\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.278218\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.078551\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.032168\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.118645\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.358507\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.363674\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.128039\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.133537\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.077445\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.172222\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.093950\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.140471\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.102340\n",
      "\n",
      "Test set: Average loss: 0.0814, Accuracy: 9740/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.047716\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.059148\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.067622\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.075810\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.100928\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.155052\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.077145\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.070464\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.283208\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.069560\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.048497\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.171632\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.097540\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.021552\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.026183\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.054637\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.100496\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.095859\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.058249\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.071105\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.180736\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.119188\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.073333\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.108026\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.043492\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.059793\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.051138\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.042728\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.105813\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.084086\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.018134\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.057006\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.032450\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.068570\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.141208\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.085988\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.044838\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.048087\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.106745\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.035386\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.030692\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.054128\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.043811\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.145837\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.169755\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.096968\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.063403\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.080706\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.143953\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.113613\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.019510\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.065102\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.100398\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.060733\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.032826\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.083125\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.090583\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.145498\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.091589\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.074400\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.084078\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.085447\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.024225\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.052612\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.027673\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.069442\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.072886\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.087097\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.056410\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.135031\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.024178\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.110713\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.016464\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.111542\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.091946\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.038885\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.097121\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.084745\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.043659\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.186802\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.123546\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.063839\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.086823\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.095019\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.055738\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.069365\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.009902\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.199493\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.020890\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.014884\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.004538\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.019860\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.096277\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.092471\n",
      "\n",
      "Test set: Average loss: 0.0536, Accuracy: 9826/10000 (98%)\n",
      "\n",
      "Breaking at epoch 2\n",
      "Files already downloaded and verified\n",
      "Evaluating MLP with CIFAR10\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.319280\n",
      "Train Epoch: 1 [640/50000 (1%)]\tLoss: 2.314381\n",
      "Train Epoch: 1 [1280/50000 (3%)]\tLoss: 2.292850\n",
      "Train Epoch: 1 [1920/50000 (4%)]\tLoss: 2.284105\n",
      "Train Epoch: 1 [2560/50000 (5%)]\tLoss: 2.299866\n",
      "Train Epoch: 1 [3200/50000 (6%)]\tLoss: 2.312349\n",
      "Train Epoch: 1 [3840/50000 (8%)]\tLoss: 2.299093\n",
      "Train Epoch: 1 [4480/50000 (9%)]\tLoss: 2.299000\n",
      "Train Epoch: 1 [5120/50000 (10%)]\tLoss: 2.288405\n",
      "Train Epoch: 1 [5760/50000 (12%)]\tLoss: 2.289542\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 2.269971\n",
      "Train Epoch: 1 [7040/50000 (14%)]\tLoss: 2.272982\n",
      "Train Epoch: 1 [7680/50000 (15%)]\tLoss: 2.282576\n",
      "Train Epoch: 1 [8320/50000 (17%)]\tLoss: 2.276575\n",
      "Train Epoch: 1 [8960/50000 (18%)]\tLoss: 2.277948\n",
      "Train Epoch: 1 [9600/50000 (19%)]\tLoss: 2.262137\n",
      "Train Epoch: 1 [10240/50000 (20%)]\tLoss: 2.228748\n",
      "Train Epoch: 1 [10880/50000 (22%)]\tLoss: 2.238303\n",
      "Train Epoch: 1 [11520/50000 (23%)]\tLoss: 2.215480\n",
      "Train Epoch: 1 [12160/50000 (24%)]\tLoss: 2.183430\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 2.255561\n",
      "Train Epoch: 1 [13440/50000 (27%)]\tLoss: 2.270692\n",
      "Train Epoch: 1 [14080/50000 (28%)]\tLoss: 2.186834\n",
      "Train Epoch: 1 [14720/50000 (29%)]\tLoss: 2.228746\n",
      "Train Epoch: 1 [15360/50000 (31%)]\tLoss: 2.229497\n",
      "Train Epoch: 1 [16000/50000 (32%)]\tLoss: 2.217835\n",
      "Train Epoch: 1 [16640/50000 (33%)]\tLoss: 2.227617\n",
      "Train Epoch: 1 [17280/50000 (35%)]\tLoss: 2.239651\n",
      "Train Epoch: 1 [17920/50000 (36%)]\tLoss: 2.198915\n",
      "Train Epoch: 1 [18560/50000 (37%)]\tLoss: 2.202352\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 2.238650\n",
      "Train Epoch: 1 [19840/50000 (40%)]\tLoss: 2.207109\n",
      "Train Epoch: 1 [20480/50000 (41%)]\tLoss: 2.258632\n",
      "Train Epoch: 1 [21120/50000 (42%)]\tLoss: 2.173290\n",
      "Train Epoch: 1 [21760/50000 (43%)]\tLoss: 2.265869\n",
      "Train Epoch: 1 [22400/50000 (45%)]\tLoss: 2.252322\n",
      "Train Epoch: 1 [23040/50000 (46%)]\tLoss: 2.232371\n",
      "Train Epoch: 1 [23680/50000 (47%)]\tLoss: 2.295232\n",
      "Train Epoch: 1 [24320/50000 (49%)]\tLoss: 2.146502\n",
      "Train Epoch: 1 [24960/50000 (50%)]\tLoss: 2.183582\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 2.199147\n",
      "Train Epoch: 1 [26240/50000 (52%)]\tLoss: 2.184241\n",
      "Train Epoch: 1 [26880/50000 (54%)]\tLoss: 2.268512\n",
      "Train Epoch: 1 [27520/50000 (55%)]\tLoss: 2.267416\n",
      "Train Epoch: 1 [28160/50000 (56%)]\tLoss: 2.194541\n",
      "Train Epoch: 1 [28800/50000 (58%)]\tLoss: 2.217190\n",
      "Train Epoch: 1 [29440/50000 (59%)]\tLoss: 2.183248\n",
      "Train Epoch: 1 [30080/50000 (60%)]\tLoss: 2.161890\n",
      "Train Epoch: 1 [30720/50000 (61%)]\tLoss: 2.156234\n",
      "Train Epoch: 1 [31360/50000 (63%)]\tLoss: 2.125397\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 2.176614\n",
      "Train Epoch: 1 [32640/50000 (65%)]\tLoss: 2.187506\n",
      "Train Epoch: 1 [33280/50000 (66%)]\tLoss: 2.103281\n",
      "Train Epoch: 1 [33920/50000 (68%)]\tLoss: 2.097283\n",
      "Train Epoch: 1 [34560/50000 (69%)]\tLoss: 2.129942\n",
      "Train Epoch: 1 [35200/50000 (70%)]\tLoss: 2.175102\n",
      "Train Epoch: 1 [35840/50000 (72%)]\tLoss: 2.196702\n",
      "Train Epoch: 1 [36480/50000 (73%)]\tLoss: 2.196203\n",
      "Train Epoch: 1 [37120/50000 (74%)]\tLoss: 2.165118\n",
      "Train Epoch: 1 [37760/50000 (75%)]\tLoss: 1.998572\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 2.180753\n",
      "Train Epoch: 1 [39040/50000 (78%)]\tLoss: 2.201053\n",
      "Train Epoch: 1 [39680/50000 (79%)]\tLoss: 2.119452\n",
      "Train Epoch: 1 [40320/50000 (81%)]\tLoss: 2.089749\n",
      "Train Epoch: 1 [40960/50000 (82%)]\tLoss: 2.154794\n",
      "Train Epoch: 1 [41600/50000 (83%)]\tLoss: 2.132364\n",
      "Train Epoch: 1 [42240/50000 (84%)]\tLoss: 2.111866\n",
      "Train Epoch: 1 [42880/50000 (86%)]\tLoss: 2.154501\n",
      "Train Epoch: 1 [43520/50000 (87%)]\tLoss: 2.046484\n",
      "Train Epoch: 1 [44160/50000 (88%)]\tLoss: 2.163046\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 2.177521\n",
      "Train Epoch: 1 [45440/50000 (91%)]\tLoss: 2.140357\n",
      "Train Epoch: 1 [46080/50000 (92%)]\tLoss: 2.099424\n",
      "Train Epoch: 1 [46720/50000 (93%)]\tLoss: 2.108118\n",
      "Train Epoch: 1 [47360/50000 (95%)]\tLoss: 2.036967\n",
      "Train Epoch: 1 [48000/50000 (96%)]\tLoss: 2.038733\n",
      "Train Epoch: 1 [48640/50000 (97%)]\tLoss: 2.021371\n",
      "Train Epoch: 1 [49280/50000 (98%)]\tLoss: 1.974335\n",
      "Train Epoch: 1 [49920/50000 (100%)]\tLoss: 2.043952\n",
      "\n",
      "Test set: Average loss: 2.0503, Accuracy: 2793/10000 (28%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 2.022041\n",
      "Train Epoch: 2 [640/50000 (1%)]\tLoss: 2.121795\n",
      "Train Epoch: 2 [1280/50000 (3%)]\tLoss: 2.045538\n",
      "Train Epoch: 2 [1920/50000 (4%)]\tLoss: 1.984984\n",
      "Train Epoch: 2 [2560/50000 (5%)]\tLoss: 2.136046\n",
      "Train Epoch: 2 [3200/50000 (6%)]\tLoss: 2.064389\n",
      "Train Epoch: 2 [3840/50000 (8%)]\tLoss: 2.064498\n",
      "Train Epoch: 2 [4480/50000 (9%)]\tLoss: 2.064377\n",
      "Train Epoch: 2 [5120/50000 (10%)]\tLoss: 2.051488\n",
      "Train Epoch: 2 [5760/50000 (12%)]\tLoss: 2.040117\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 2.141221\n",
      "Train Epoch: 2 [7040/50000 (14%)]\tLoss: 2.029631\n",
      "Train Epoch: 2 [7680/50000 (15%)]\tLoss: 2.063335\n",
      "Train Epoch: 2 [8320/50000 (17%)]\tLoss: 2.046731\n",
      "Train Epoch: 2 [8960/50000 (18%)]\tLoss: 2.118987\n",
      "Train Epoch: 2 [9600/50000 (19%)]\tLoss: 2.068599\n",
      "Train Epoch: 2 [10240/50000 (20%)]\tLoss: 2.103414\n",
      "Train Epoch: 2 [10880/50000 (22%)]\tLoss: 2.144411\n",
      "Train Epoch: 2 [11520/50000 (23%)]\tLoss: 1.991227\n",
      "Train Epoch: 2 [12160/50000 (24%)]\tLoss: 2.022459\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 2.099379\n",
      "Train Epoch: 2 [13440/50000 (27%)]\tLoss: 2.059891\n",
      "Train Epoch: 2 [14080/50000 (28%)]\tLoss: 2.018191\n",
      "Train Epoch: 2 [14720/50000 (29%)]\tLoss: 2.043784\n",
      "Train Epoch: 2 [15360/50000 (31%)]\tLoss: 2.070660\n",
      "Train Epoch: 2 [16000/50000 (32%)]\tLoss: 2.109310\n",
      "Train Epoch: 2 [16640/50000 (33%)]\tLoss: 2.240547\n",
      "Train Epoch: 2 [17280/50000 (35%)]\tLoss: 2.075977\n",
      "Train Epoch: 2 [17920/50000 (36%)]\tLoss: 2.019932\n",
      "Train Epoch: 2 [18560/50000 (37%)]\tLoss: 1.918584\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 1.962248\n",
      "Train Epoch: 2 [19840/50000 (40%)]\tLoss: 2.033751\n",
      "Train Epoch: 2 [20480/50000 (41%)]\tLoss: 1.969406\n",
      "Train Epoch: 2 [21120/50000 (42%)]\tLoss: 1.989861\n",
      "Train Epoch: 2 [21760/50000 (43%)]\tLoss: 2.029830\n",
      "Train Epoch: 2 [22400/50000 (45%)]\tLoss: 2.088206\n",
      "Train Epoch: 2 [23040/50000 (46%)]\tLoss: 1.982208\n",
      "Train Epoch: 2 [23680/50000 (47%)]\tLoss: 2.097285\n",
      "Train Epoch: 2 [24320/50000 (49%)]\tLoss: 2.172739\n",
      "Train Epoch: 2 [24960/50000 (50%)]\tLoss: 2.091621\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 2.054466\n",
      "Train Epoch: 2 [26240/50000 (52%)]\tLoss: 1.923084\n",
      "Train Epoch: 2 [26880/50000 (54%)]\tLoss: 2.065329\n",
      "Train Epoch: 2 [27520/50000 (55%)]\tLoss: 1.978181\n",
      "Train Epoch: 2 [28160/50000 (56%)]\tLoss: 1.982686\n",
      "Train Epoch: 2 [28800/50000 (58%)]\tLoss: 1.923154\n",
      "Train Epoch: 2 [29440/50000 (59%)]\tLoss: 1.897587\n",
      "Train Epoch: 2 [30080/50000 (60%)]\tLoss: 1.879731\n",
      "Train Epoch: 2 [30720/50000 (61%)]\tLoss: 1.957040\n",
      "Train Epoch: 2 [31360/50000 (63%)]\tLoss: 2.114342\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 2.107453\n",
      "Train Epoch: 2 [32640/50000 (65%)]\tLoss: 2.093821\n",
      "Train Epoch: 2 [33280/50000 (66%)]\tLoss: 2.072584\n",
      "Train Epoch: 2 [33920/50000 (68%)]\tLoss: 1.938348\n",
      "Train Epoch: 2 [34560/50000 (69%)]\tLoss: 2.075440\n",
      "Train Epoch: 2 [35200/50000 (70%)]\tLoss: 2.025356\n",
      "Train Epoch: 2 [35840/50000 (72%)]\tLoss: 2.010384\n",
      "Train Epoch: 2 [36480/50000 (73%)]\tLoss: 2.158416\n",
      "Train Epoch: 2 [37120/50000 (74%)]\tLoss: 2.050095\n",
      "Train Epoch: 2 [37760/50000 (75%)]\tLoss: 2.020969\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 1.982895\n",
      "Train Epoch: 2 [39040/50000 (78%)]\tLoss: 1.944203\n",
      "Train Epoch: 2 [39680/50000 (79%)]\tLoss: 1.977095\n",
      "Train Epoch: 2 [40320/50000 (81%)]\tLoss: 2.020535\n",
      "Train Epoch: 2 [40960/50000 (82%)]\tLoss: 1.954268\n",
      "Train Epoch: 2 [41600/50000 (83%)]\tLoss: 2.035044\n",
      "Train Epoch: 2 [42240/50000 (84%)]\tLoss: 2.098897\n",
      "Train Epoch: 2 [42880/50000 (86%)]\tLoss: 1.943441\n",
      "Train Epoch: 2 [43520/50000 (87%)]\tLoss: 1.956543\n",
      "Train Epoch: 2 [44160/50000 (88%)]\tLoss: 2.047096\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 2.100461\n",
      "Train Epoch: 2 [45440/50000 (91%)]\tLoss: 2.046179\n",
      "Train Epoch: 2 [46080/50000 (92%)]\tLoss: 2.044184\n",
      "Train Epoch: 2 [46720/50000 (93%)]\tLoss: 2.113240\n",
      "Train Epoch: 2 [47360/50000 (95%)]\tLoss: 1.884432\n",
      "Train Epoch: 2 [48000/50000 (96%)]\tLoss: 1.920928\n",
      "Train Epoch: 2 [48640/50000 (97%)]\tLoss: 2.076423\n",
      "Train Epoch: 2 [49280/50000 (98%)]\tLoss: 2.033837\n",
      "Train Epoch: 2 [49920/50000 (100%)]\tLoss: 1.962438\n",
      "\n",
      "Test set: Average loss: 1.9446, Accuracy: 3077/10000 (31%)\n",
      "\n",
      "Evaluating CNN with CIFAR10\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.293661\n",
      "Train Epoch: 1 [640/50000 (1%)]\tLoss: 2.298783\n",
      "Train Epoch: 1 [1280/50000 (3%)]\tLoss: 2.290471\n",
      "Train Epoch: 1 [1920/50000 (4%)]\tLoss: 2.318704\n",
      "Train Epoch: 1 [2560/50000 (5%)]\tLoss: 2.280287\n",
      "Train Epoch: 1 [3200/50000 (6%)]\tLoss: 2.320403\n",
      "Train Epoch: 1 [3840/50000 (8%)]\tLoss: 2.289097\n",
      "Train Epoch: 1 [4480/50000 (9%)]\tLoss: 2.302276\n",
      "Train Epoch: 1 [5120/50000 (10%)]\tLoss: 2.297284\n",
      "Train Epoch: 1 [5760/50000 (12%)]\tLoss: 2.293231\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 2.290737\n",
      "Train Epoch: 1 [7040/50000 (14%)]\tLoss: 2.280202\n",
      "Train Epoch: 1 [7680/50000 (15%)]\tLoss: 2.284465\n",
      "Train Epoch: 1 [8320/50000 (17%)]\tLoss: 2.280303\n",
      "Train Epoch: 1 [8960/50000 (18%)]\tLoss: 2.292749\n",
      "Train Epoch: 1 [9600/50000 (19%)]\tLoss: 2.289855\n",
      "Train Epoch: 1 [10240/50000 (20%)]\tLoss: 2.302312\n",
      "Train Epoch: 1 [10880/50000 (22%)]\tLoss: 2.268149\n",
      "Train Epoch: 1 [11520/50000 (23%)]\tLoss: 2.286303\n",
      "Train Epoch: 1 [12160/50000 (24%)]\tLoss: 2.265178\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 2.257725\n",
      "Train Epoch: 1 [13440/50000 (27%)]\tLoss: 2.270886\n",
      "Train Epoch: 1 [14080/50000 (28%)]\tLoss: 2.255984\n",
      "Train Epoch: 1 [14720/50000 (29%)]\tLoss: 2.262166\n",
      "Train Epoch: 1 [15360/50000 (31%)]\tLoss: 2.246734\n",
      "Train Epoch: 1 [16000/50000 (32%)]\tLoss: 2.237321\n",
      "Train Epoch: 1 [16640/50000 (33%)]\tLoss: 2.223917\n",
      "Train Epoch: 1 [17280/50000 (35%)]\tLoss: 2.272171\n",
      "Train Epoch: 1 [17920/50000 (36%)]\tLoss: 2.240761\n",
      "Train Epoch: 1 [18560/50000 (37%)]\tLoss: 2.249190\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 2.205393\n",
      "Train Epoch: 1 [19840/50000 (40%)]\tLoss: 2.195704\n",
      "Train Epoch: 1 [20480/50000 (41%)]\tLoss: 2.179377\n",
      "Train Epoch: 1 [21120/50000 (42%)]\tLoss: 2.156423\n",
      "Train Epoch: 1 [21760/50000 (43%)]\tLoss: 2.207482\n",
      "Train Epoch: 1 [22400/50000 (45%)]\tLoss: 2.249313\n",
      "Train Epoch: 1 [23040/50000 (46%)]\tLoss: 2.232669\n",
      "Train Epoch: 1 [23680/50000 (47%)]\tLoss: 2.170156\n",
      "Train Epoch: 1 [24320/50000 (49%)]\tLoss: 2.190794\n",
      "Train Epoch: 1 [24960/50000 (50%)]\tLoss: 2.155610\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 2.090315\n",
      "Train Epoch: 1 [26240/50000 (52%)]\tLoss: 2.192581\n",
      "Train Epoch: 1 [26880/50000 (54%)]\tLoss: 2.248325\n",
      "Train Epoch: 1 [27520/50000 (55%)]\tLoss: 2.213813\n",
      "Train Epoch: 1 [28160/50000 (56%)]\tLoss: 2.087373\n",
      "Train Epoch: 1 [28800/50000 (58%)]\tLoss: 2.089836\n",
      "Train Epoch: 1 [29440/50000 (59%)]\tLoss: 2.122533\n",
      "Train Epoch: 1 [30080/50000 (60%)]\tLoss: 2.076613\n",
      "Train Epoch: 1 [30720/50000 (61%)]\tLoss: 2.136031\n",
      "Train Epoch: 1 [31360/50000 (63%)]\tLoss: 2.106719\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 2.145755\n",
      "Train Epoch: 1 [32640/50000 (65%)]\tLoss: 2.042262\n",
      "Train Epoch: 1 [33280/50000 (66%)]\tLoss: 2.060903\n",
      "Train Epoch: 1 [33920/50000 (68%)]\tLoss: 2.065791\n",
      "Train Epoch: 1 [34560/50000 (69%)]\tLoss: 2.033648\n",
      "Train Epoch: 1 [35200/50000 (70%)]\tLoss: 2.041711\n",
      "Train Epoch: 1 [35840/50000 (72%)]\tLoss: 1.944502\n",
      "Train Epoch: 1 [36480/50000 (73%)]\tLoss: 2.045332\n",
      "Train Epoch: 1 [37120/50000 (74%)]\tLoss: 2.031877\n",
      "Train Epoch: 1 [37760/50000 (75%)]\tLoss: 2.054700\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 2.082591\n",
      "Train Epoch: 1 [39040/50000 (78%)]\tLoss: 2.012156\n",
      "Train Epoch: 1 [39680/50000 (79%)]\tLoss: 2.101122\n",
      "Train Epoch: 1 [40320/50000 (81%)]\tLoss: 2.063272\n",
      "Train Epoch: 1 [40960/50000 (82%)]\tLoss: 2.016874\n",
      "Train Epoch: 1 [41600/50000 (83%)]\tLoss: 2.008512\n",
      "Train Epoch: 1 [42240/50000 (84%)]\tLoss: 2.044643\n",
      "Train Epoch: 1 [42880/50000 (86%)]\tLoss: 2.127689\n",
      "Train Epoch: 1 [43520/50000 (87%)]\tLoss: 2.005979\n",
      "Train Epoch: 1 [44160/50000 (88%)]\tLoss: 2.104755\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 1.980512\n",
      "Train Epoch: 1 [45440/50000 (91%)]\tLoss: 1.946314\n",
      "Train Epoch: 1 [46080/50000 (92%)]\tLoss: 1.974134\n",
      "Train Epoch: 1 [46720/50000 (93%)]\tLoss: 2.094509\n",
      "Train Epoch: 1 [47360/50000 (95%)]\tLoss: 1.950865\n",
      "Train Epoch: 1 [48000/50000 (96%)]\tLoss: 1.858990\n",
      "Train Epoch: 1 [48640/50000 (97%)]\tLoss: 2.003238\n",
      "Train Epoch: 1 [49280/50000 (98%)]\tLoss: 2.042113\n",
      "Train Epoch: 1 [49920/50000 (100%)]\tLoss: 2.004698\n",
      "\n",
      "Test set: Average loss: 2.0031, Accuracy: 3193/10000 (32%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.937431\n",
      "Train Epoch: 2 [640/50000 (1%)]\tLoss: 2.029907\n",
      "Train Epoch: 2 [1280/50000 (3%)]\tLoss: 2.089628\n",
      "Train Epoch: 2 [1920/50000 (4%)]\tLoss: 1.934227\n",
      "Train Epoch: 2 [2560/50000 (5%)]\tLoss: 1.978373\n",
      "Train Epoch: 2 [3200/50000 (6%)]\tLoss: 1.951688\n",
      "Train Epoch: 2 [3840/50000 (8%)]\tLoss: 1.996425\n",
      "Train Epoch: 2 [4480/50000 (9%)]\tLoss: 1.904220\n",
      "Train Epoch: 2 [5120/50000 (10%)]\tLoss: 1.986685\n",
      "Train Epoch: 2 [5760/50000 (12%)]\tLoss: 1.962819\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 1.925521\n",
      "Train Epoch: 2 [7040/50000 (14%)]\tLoss: 2.075475\n",
      "Train Epoch: 2 [7680/50000 (15%)]\tLoss: 1.879573\n",
      "Train Epoch: 2 [8320/50000 (17%)]\tLoss: 1.850195\n",
      "Train Epoch: 2 [8960/50000 (18%)]\tLoss: 1.945605\n",
      "Train Epoch: 2 [9600/50000 (19%)]\tLoss: 1.963255\n",
      "Train Epoch: 2 [10240/50000 (20%)]\tLoss: 1.915960\n",
      "Train Epoch: 2 [10880/50000 (22%)]\tLoss: 1.973799\n",
      "Train Epoch: 2 [11520/50000 (23%)]\tLoss: 1.938079\n",
      "Train Epoch: 2 [12160/50000 (24%)]\tLoss: 1.909211\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 1.996947\n",
      "Train Epoch: 2 [13440/50000 (27%)]\tLoss: 1.978996\n",
      "Train Epoch: 2 [14080/50000 (28%)]\tLoss: 1.957669\n",
      "Train Epoch: 2 [14720/50000 (29%)]\tLoss: 1.896072\n",
      "Train Epoch: 2 [15360/50000 (31%)]\tLoss: 2.121649\n",
      "Train Epoch: 2 [16000/50000 (32%)]\tLoss: 1.901834\n",
      "Train Epoch: 2 [16640/50000 (33%)]\tLoss: 1.951611\n",
      "Train Epoch: 2 [17280/50000 (35%)]\tLoss: 1.900747\n",
      "Train Epoch: 2 [17920/50000 (36%)]\tLoss: 2.041113\n",
      "Train Epoch: 2 [18560/50000 (37%)]\tLoss: 1.866410\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 2.073834\n",
      "Train Epoch: 2 [19840/50000 (40%)]\tLoss: 1.943445\n",
      "Train Epoch: 2 [20480/50000 (41%)]\tLoss: 1.904623\n",
      "Train Epoch: 2 [21120/50000 (42%)]\tLoss: 2.055874\n",
      "Train Epoch: 2 [21760/50000 (43%)]\tLoss: 1.824657\n",
      "Train Epoch: 2 [22400/50000 (45%)]\tLoss: 1.811809\n",
      "Train Epoch: 2 [23040/50000 (46%)]\tLoss: 1.929371\n",
      "Train Epoch: 2 [23680/50000 (47%)]\tLoss: 1.862555\n",
      "Train Epoch: 2 [24320/50000 (49%)]\tLoss: 1.920354\n",
      "Train Epoch: 2 [24960/50000 (50%)]\tLoss: 1.970380\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 2.023093\n",
      "Train Epoch: 2 [26240/50000 (52%)]\tLoss: 2.080553\n",
      "Train Epoch: 2 [26880/50000 (54%)]\tLoss: 1.932387\n",
      "Train Epoch: 2 [27520/50000 (55%)]\tLoss: 2.003108\n",
      "Train Epoch: 2 [28160/50000 (56%)]\tLoss: 2.003245\n",
      "Train Epoch: 2 [28800/50000 (58%)]\tLoss: 1.915239\n",
      "Train Epoch: 2 [29440/50000 (59%)]\tLoss: 1.877798\n",
      "Train Epoch: 2 [30080/50000 (60%)]\tLoss: 2.114373\n",
      "Train Epoch: 2 [30720/50000 (61%)]\tLoss: 1.948117\n",
      "Train Epoch: 2 [31360/50000 (63%)]\tLoss: 1.889747\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 2.042152\n",
      "Train Epoch: 2 [32640/50000 (65%)]\tLoss: 1.961174\n",
      "Train Epoch: 2 [33280/50000 (66%)]\tLoss: 1.787918\n",
      "Train Epoch: 2 [33920/50000 (68%)]\tLoss: 1.986238\n",
      "Train Epoch: 2 [34560/50000 (69%)]\tLoss: 1.880214\n",
      "Train Epoch: 2 [35200/50000 (70%)]\tLoss: 1.888682\n",
      "Train Epoch: 2 [35840/50000 (72%)]\tLoss: 1.889456\n",
      "Train Epoch: 2 [36480/50000 (73%)]\tLoss: 1.848315\n",
      "Train Epoch: 2 [37120/50000 (74%)]\tLoss: 1.865845\n",
      "Train Epoch: 2 [37760/50000 (75%)]\tLoss: 2.025318\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 1.896374\n",
      "Train Epoch: 2 [39040/50000 (78%)]\tLoss: 1.792729\n",
      "Train Epoch: 2 [39680/50000 (79%)]\tLoss: 1.847411\n",
      "Train Epoch: 2 [40320/50000 (81%)]\tLoss: 2.015334\n",
      "Train Epoch: 2 [40960/50000 (82%)]\tLoss: 1.982790\n",
      "Train Epoch: 2 [41600/50000 (83%)]\tLoss: 1.835616\n",
      "Train Epoch: 2 [42240/50000 (84%)]\tLoss: 1.858955\n",
      "Train Epoch: 2 [42880/50000 (86%)]\tLoss: 1.881545\n",
      "Train Epoch: 2 [43520/50000 (87%)]\tLoss: 1.840802\n",
      "Train Epoch: 2 [44160/50000 (88%)]\tLoss: 1.832239\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 1.903138\n",
      "Train Epoch: 2 [45440/50000 (91%)]\tLoss: 1.883428\n",
      "Train Epoch: 2 [46080/50000 (92%)]\tLoss: 1.882527\n",
      "Train Epoch: 2 [46720/50000 (93%)]\tLoss: 1.932364\n",
      "Train Epoch: 2 [47360/50000 (95%)]\tLoss: 2.043793\n",
      "Train Epoch: 2 [48000/50000 (96%)]\tLoss: 1.903341\n",
      "Train Epoch: 2 [48640/50000 (97%)]\tLoss: 1.919414\n",
      "Train Epoch: 2 [49280/50000 (98%)]\tLoss: 1.774953\n",
      "Train Epoch: 2 [49920/50000 (100%)]\tLoss: 1.880606\n",
      "\n",
      "Test set: Average loss: 1.8941, Accuracy: 3506/10000 (35%)\n",
      "\n",
      "Evaluating ViT with CIFAR10\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.410791\n",
      "Train Epoch: 1 [640/50000 (1%)]\tLoss: 2.310933\n",
      "Train Epoch: 1 [1280/50000 (3%)]\tLoss: 2.219033\n",
      "Train Epoch: 1 [1920/50000 (4%)]\tLoss: 2.193349\n",
      "Train Epoch: 1 [2560/50000 (5%)]\tLoss: 2.055856\n",
      "Train Epoch: 1 [3200/50000 (6%)]\tLoss: 1.995741\n",
      "Train Epoch: 1 [3840/50000 (8%)]\tLoss: 2.043393\n",
      "Train Epoch: 1 [4480/50000 (9%)]\tLoss: 2.024985\n",
      "Train Epoch: 1 [5120/50000 (10%)]\tLoss: 2.066596\n",
      "Train Epoch: 1 [5760/50000 (12%)]\tLoss: 2.051566\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 1.957848\n",
      "Train Epoch: 1 [7040/50000 (14%)]\tLoss: 2.066250\n",
      "Train Epoch: 1 [7680/50000 (15%)]\tLoss: 1.935160\n",
      "Train Epoch: 1 [8320/50000 (17%)]\tLoss: 2.002340\n",
      "Train Epoch: 1 [8960/50000 (18%)]\tLoss: 1.953645\n",
      "Train Epoch: 1 [9600/50000 (19%)]\tLoss: 2.016181\n",
      "Train Epoch: 1 [10240/50000 (20%)]\tLoss: 2.033399\n",
      "Train Epoch: 1 [10880/50000 (22%)]\tLoss: 1.905135\n",
      "Train Epoch: 1 [11520/50000 (23%)]\tLoss: 1.991870\n",
      "Train Epoch: 1 [12160/50000 (24%)]\tLoss: 1.932375\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 1.833799\n",
      "Train Epoch: 1 [13440/50000 (27%)]\tLoss: 1.718857\n",
      "Train Epoch: 1 [14080/50000 (28%)]\tLoss: 1.848739\n",
      "Train Epoch: 1 [14720/50000 (29%)]\tLoss: 1.720441\n",
      "Train Epoch: 1 [15360/50000 (31%)]\tLoss: 1.880177\n",
      "Train Epoch: 1 [16000/50000 (32%)]\tLoss: 1.873073\n",
      "Train Epoch: 1 [16640/50000 (33%)]\tLoss: 1.620229\n",
      "Train Epoch: 1 [17280/50000 (35%)]\tLoss: 1.767377\n",
      "Train Epoch: 1 [17920/50000 (36%)]\tLoss: 1.839962\n",
      "Train Epoch: 1 [18560/50000 (37%)]\tLoss: 1.949519\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 1.738561\n",
      "Train Epoch: 1 [19840/50000 (40%)]\tLoss: 2.097476\n",
      "Train Epoch: 1 [20480/50000 (41%)]\tLoss: 1.784419\n",
      "Train Epoch: 1 [21120/50000 (42%)]\tLoss: 1.772682\n",
      "Train Epoch: 1 [21760/50000 (43%)]\tLoss: 1.892403\n",
      "Train Epoch: 1 [22400/50000 (45%)]\tLoss: 1.679258\n",
      "Train Epoch: 1 [23040/50000 (46%)]\tLoss: 1.915117\n",
      "Train Epoch: 1 [23680/50000 (47%)]\tLoss: 1.779799\n",
      "Train Epoch: 1 [24320/50000 (49%)]\tLoss: 1.961155\n",
      "Train Epoch: 1 [24960/50000 (50%)]\tLoss: 1.554108\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 1.605604\n",
      "Train Epoch: 1 [26240/50000 (52%)]\tLoss: 1.840130\n",
      "Train Epoch: 1 [26880/50000 (54%)]\tLoss: 1.776549\n",
      "Train Epoch: 1 [27520/50000 (55%)]\tLoss: 1.836158\n",
      "Train Epoch: 1 [28160/50000 (56%)]\tLoss: 1.594102\n",
      "Train Epoch: 1 [28800/50000 (58%)]\tLoss: 1.839938\n",
      "Train Epoch: 1 [29440/50000 (59%)]\tLoss: 1.593275\n",
      "Train Epoch: 1 [30080/50000 (60%)]\tLoss: 1.824206\n",
      "Train Epoch: 1 [30720/50000 (61%)]\tLoss: 1.902072\n",
      "Train Epoch: 1 [31360/50000 (63%)]\tLoss: 1.933734\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 1.917464\n",
      "Train Epoch: 1 [32640/50000 (65%)]\tLoss: 1.928639\n",
      "Train Epoch: 1 [33280/50000 (66%)]\tLoss: 1.754867\n",
      "Train Epoch: 1 [33920/50000 (68%)]\tLoss: 1.654116\n",
      "Train Epoch: 1 [34560/50000 (69%)]\tLoss: 1.728493\n",
      "Train Epoch: 1 [35200/50000 (70%)]\tLoss: 1.751101\n",
      "Train Epoch: 1 [35840/50000 (72%)]\tLoss: 1.776119\n",
      "Train Epoch: 1 [36480/50000 (73%)]\tLoss: 1.675673\n",
      "Train Epoch: 1 [37120/50000 (74%)]\tLoss: 1.830742\n",
      "Train Epoch: 1 [37760/50000 (75%)]\tLoss: 1.808271\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 1.617618\n",
      "Train Epoch: 1 [39040/50000 (78%)]\tLoss: 1.586790\n",
      "Train Epoch: 1 [39680/50000 (79%)]\tLoss: 1.796448\n",
      "Train Epoch: 1 [40320/50000 (81%)]\tLoss: 1.731654\n",
      "Train Epoch: 1 [40960/50000 (82%)]\tLoss: 1.802757\n",
      "Train Epoch: 1 [41600/50000 (83%)]\tLoss: 1.805442\n",
      "Train Epoch: 1 [42240/50000 (84%)]\tLoss: 1.839904\n",
      "Train Epoch: 1 [42880/50000 (86%)]\tLoss: 1.793674\n",
      "Train Epoch: 1 [43520/50000 (87%)]\tLoss: 1.588371\n",
      "Train Epoch: 1 [44160/50000 (88%)]\tLoss: 1.996750\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 1.697762\n",
      "Train Epoch: 1 [45440/50000 (91%)]\tLoss: 1.804883\n",
      "Train Epoch: 1 [46080/50000 (92%)]\tLoss: 1.673425\n",
      "Train Epoch: 1 [46720/50000 (93%)]\tLoss: 1.734728\n",
      "Train Epoch: 1 [47360/50000 (95%)]\tLoss: 1.620512\n",
      "Train Epoch: 1 [48000/50000 (96%)]\tLoss: 1.975951\n",
      "Train Epoch: 1 [48640/50000 (97%)]\tLoss: 1.591121\n",
      "Train Epoch: 1 [49280/50000 (98%)]\tLoss: 1.613483\n",
      "Train Epoch: 1 [49920/50000 (100%)]\tLoss: 1.807782\n",
      "\n",
      "Test set: Average loss: 1.6799, Accuracy: 4127/10000 (41%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.784379\n",
      "Train Epoch: 2 [640/50000 (1%)]\tLoss: 1.742769\n",
      "Train Epoch: 2 [1280/50000 (3%)]\tLoss: 1.768416\n",
      "Train Epoch: 2 [1920/50000 (4%)]\tLoss: 1.742301\n",
      "Train Epoch: 2 [2560/50000 (5%)]\tLoss: 1.587973\n",
      "Train Epoch: 2 [3200/50000 (6%)]\tLoss: 1.627948\n",
      "Train Epoch: 2 [3840/50000 (8%)]\tLoss: 1.715979\n",
      "Train Epoch: 2 [4480/50000 (9%)]\tLoss: 1.773226\n",
      "Train Epoch: 2 [5120/50000 (10%)]\tLoss: 1.604005\n",
      "Train Epoch: 2 [5760/50000 (12%)]\tLoss: 1.808403\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 1.710043\n",
      "Train Epoch: 2 [7040/50000 (14%)]\tLoss: 1.575768\n",
      "Train Epoch: 2 [7680/50000 (15%)]\tLoss: 1.476539\n",
      "Train Epoch: 2 [8320/50000 (17%)]\tLoss: 1.480171\n",
      "Train Epoch: 2 [8960/50000 (18%)]\tLoss: 1.769286\n",
      "Train Epoch: 2 [9600/50000 (19%)]\tLoss: 1.745371\n",
      "Train Epoch: 2 [10240/50000 (20%)]\tLoss: 1.624550\n",
      "Train Epoch: 2 [10880/50000 (22%)]\tLoss: 1.777524\n",
      "Train Epoch: 2 [11520/50000 (23%)]\tLoss: 1.452420\n",
      "Train Epoch: 2 [12160/50000 (24%)]\tLoss: 1.917531\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 1.778826\n",
      "Train Epoch: 2 [13440/50000 (27%)]\tLoss: 1.632938\n",
      "Train Epoch: 2 [14080/50000 (28%)]\tLoss: 1.512030\n",
      "Train Epoch: 2 [14720/50000 (29%)]\tLoss: 1.813959\n",
      "Train Epoch: 2 [15360/50000 (31%)]\tLoss: 1.600273\n",
      "Train Epoch: 2 [16000/50000 (32%)]\tLoss: 1.883449\n",
      "Train Epoch: 2 [16640/50000 (33%)]\tLoss: 1.752856\n",
      "Train Epoch: 2 [17280/50000 (35%)]\tLoss: 1.452638\n",
      "Train Epoch: 2 [17920/50000 (36%)]\tLoss: 1.855959\n",
      "Train Epoch: 2 [18560/50000 (37%)]\tLoss: 1.696917\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 1.423499\n",
      "Train Epoch: 2 [19840/50000 (40%)]\tLoss: 1.483947\n",
      "Train Epoch: 2 [20480/50000 (41%)]\tLoss: 1.572887\n",
      "Train Epoch: 2 [21120/50000 (42%)]\tLoss: 1.439683\n",
      "Train Epoch: 2 [21760/50000 (43%)]\tLoss: 1.467439\n",
      "Train Epoch: 2 [22400/50000 (45%)]\tLoss: 1.679043\n",
      "Train Epoch: 2 [23040/50000 (46%)]\tLoss: 1.779174\n",
      "Train Epoch: 2 [23680/50000 (47%)]\tLoss: 1.417376\n",
      "Train Epoch: 2 [24320/50000 (49%)]\tLoss: 1.522947\n",
      "Train Epoch: 2 [24960/50000 (50%)]\tLoss: 1.488896\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 1.594998\n",
      "Train Epoch: 2 [26240/50000 (52%)]\tLoss: 1.534537\n",
      "Train Epoch: 2 [26880/50000 (54%)]\tLoss: 1.840878\n",
      "Train Epoch: 2 [27520/50000 (55%)]\tLoss: 1.529396\n",
      "Train Epoch: 2 [28160/50000 (56%)]\tLoss: 1.528774\n",
      "Train Epoch: 2 [28800/50000 (58%)]\tLoss: 1.702323\n",
      "Train Epoch: 2 [29440/50000 (59%)]\tLoss: 1.674897\n",
      "Train Epoch: 2 [30080/50000 (60%)]\tLoss: 1.637687\n",
      "Train Epoch: 2 [30720/50000 (61%)]\tLoss: 1.464122\n",
      "Train Epoch: 2 [31360/50000 (63%)]\tLoss: 1.575112\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 1.652993\n",
      "Train Epoch: 2 [32640/50000 (65%)]\tLoss: 1.474413\n",
      "Train Epoch: 2 [33280/50000 (66%)]\tLoss: 1.481507\n",
      "Train Epoch: 2 [33920/50000 (68%)]\tLoss: 1.470530\n",
      "Train Epoch: 2 [34560/50000 (69%)]\tLoss: 1.558564\n",
      "Train Epoch: 2 [35200/50000 (70%)]\tLoss: 1.679915\n",
      "Train Epoch: 2 [35840/50000 (72%)]\tLoss: 1.759792\n",
      "Train Epoch: 2 [36480/50000 (73%)]\tLoss: 1.604329\n",
      "Train Epoch: 2 [37120/50000 (74%)]\tLoss: 1.637723\n",
      "Train Epoch: 2 [37760/50000 (75%)]\tLoss: 1.783934\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 1.590504\n",
      "Train Epoch: 2 [39040/50000 (78%)]\tLoss: 1.465548\n",
      "Train Epoch: 2 [39680/50000 (79%)]\tLoss: 1.621576\n",
      "Train Epoch: 2 [40320/50000 (81%)]\tLoss: 1.901494\n",
      "Train Epoch: 2 [40960/50000 (82%)]\tLoss: 1.717187\n",
      "Train Epoch: 2 [41600/50000 (83%)]\tLoss: 1.577576\n",
      "Train Epoch: 2 [42240/50000 (84%)]\tLoss: 1.390762\n",
      "Train Epoch: 2 [42880/50000 (86%)]\tLoss: 1.587687\n",
      "Train Epoch: 2 [43520/50000 (87%)]\tLoss: 1.671977\n",
      "Train Epoch: 2 [44160/50000 (88%)]\tLoss: 1.520683\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 1.708470\n",
      "Train Epoch: 2 [45440/50000 (91%)]\tLoss: 1.956293\n",
      "Train Epoch: 2 [46080/50000 (92%)]\tLoss: 1.447781\n",
      "Train Epoch: 2 [46720/50000 (93%)]\tLoss: 1.600107\n",
      "Train Epoch: 2 [47360/50000 (95%)]\tLoss: 1.444265\n",
      "Train Epoch: 2 [48000/50000 (96%)]\tLoss: 1.552074\n",
      "Train Epoch: 2 [48640/50000 (97%)]\tLoss: 1.529843\n",
      "Train Epoch: 2 [49280/50000 (98%)]\tLoss: 1.768303\n",
      "Train Epoch: 2 [49920/50000 (100%)]\tLoss: 1.465725\n",
      "\n",
      "Test set: Average loss: 1.5993, Accuracy: 4427/10000 (44%)\n",
      "\n",
      "Evaluating HYBRID with CIFAR10\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.506618\n",
      "Train Epoch: 1 [640/50000 (1%)]\tLoss: 2.370980\n",
      "Train Epoch: 1 [1280/50000 (3%)]\tLoss: 2.234981\n",
      "Train Epoch: 1 [1920/50000 (4%)]\tLoss: 2.225448\n",
      "Train Epoch: 1 [2560/50000 (5%)]\tLoss: 2.232514\n",
      "Train Epoch: 1 [3200/50000 (6%)]\tLoss: 2.080417\n",
      "Train Epoch: 1 [3840/50000 (8%)]\tLoss: 2.038709\n",
      "Train Epoch: 1 [4480/50000 (9%)]\tLoss: 2.077929\n",
      "Train Epoch: 1 [5120/50000 (10%)]\tLoss: 2.113097\n",
      "Train Epoch: 1 [5760/50000 (12%)]\tLoss: 1.960592\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 1.858890\n",
      "Train Epoch: 1 [7040/50000 (14%)]\tLoss: 1.866838\n",
      "Train Epoch: 1 [7680/50000 (15%)]\tLoss: 1.734369\n",
      "Train Epoch: 1 [8320/50000 (17%)]\tLoss: 2.019521\n",
      "Train Epoch: 1 [8960/50000 (18%)]\tLoss: 1.945581\n",
      "Train Epoch: 1 [9600/50000 (19%)]\tLoss: 1.865781\n",
      "Train Epoch: 1 [10240/50000 (20%)]\tLoss: 2.040252\n",
      "Train Epoch: 1 [10880/50000 (22%)]\tLoss: 1.744800\n",
      "Train Epoch: 1 [11520/50000 (23%)]\tLoss: 1.831456\n",
      "Train Epoch: 1 [12160/50000 (24%)]\tLoss: 1.696655\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 1.663273\n",
      "Train Epoch: 1 [13440/50000 (27%)]\tLoss: 1.952543\n",
      "Train Epoch: 1 [14080/50000 (28%)]\tLoss: 1.726489\n",
      "Train Epoch: 1 [14720/50000 (29%)]\tLoss: 1.986917\n",
      "Train Epoch: 1 [15360/50000 (31%)]\tLoss: 1.779038\n",
      "Train Epoch: 1 [16000/50000 (32%)]\tLoss: 1.747622\n",
      "Train Epoch: 1 [16640/50000 (33%)]\tLoss: 1.741688\n",
      "Train Epoch: 1 [17280/50000 (35%)]\tLoss: 1.611653\n",
      "Train Epoch: 1 [17920/50000 (36%)]\tLoss: 1.695557\n",
      "Train Epoch: 1 [18560/50000 (37%)]\tLoss: 1.622159\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 1.702226\n",
      "Train Epoch: 1 [19840/50000 (40%)]\tLoss: 1.672322\n",
      "Train Epoch: 1 [20480/50000 (41%)]\tLoss: 1.816793\n",
      "Train Epoch: 1 [21120/50000 (42%)]\tLoss: 1.657124\n",
      "Train Epoch: 1 [21760/50000 (43%)]\tLoss: 1.612295\n",
      "Train Epoch: 1 [22400/50000 (45%)]\tLoss: 1.704174\n",
      "Train Epoch: 1 [23040/50000 (46%)]\tLoss: 1.510989\n",
      "Train Epoch: 1 [23680/50000 (47%)]\tLoss: 1.607795\n",
      "Train Epoch: 1 [24320/50000 (49%)]\tLoss: 1.355898\n",
      "Train Epoch: 1 [24960/50000 (50%)]\tLoss: 1.637644\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 1.514577\n",
      "Train Epoch: 1 [26240/50000 (52%)]\tLoss: 1.517990\n",
      "Train Epoch: 1 [26880/50000 (54%)]\tLoss: 1.586851\n",
      "Train Epoch: 1 [27520/50000 (55%)]\tLoss: 1.566823\n",
      "Train Epoch: 1 [28160/50000 (56%)]\tLoss: 1.376330\n",
      "Train Epoch: 1 [28800/50000 (58%)]\tLoss: 1.720507\n",
      "Train Epoch: 1 [29440/50000 (59%)]\tLoss: 1.506851\n",
      "Train Epoch: 1 [30080/50000 (60%)]\tLoss: 1.541426\n",
      "Train Epoch: 1 [30720/50000 (61%)]\tLoss: 1.682317\n",
      "Train Epoch: 1 [31360/50000 (63%)]\tLoss: 1.338674\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 1.791889\n",
      "Train Epoch: 1 [32640/50000 (65%)]\tLoss: 1.620781\n",
      "Train Epoch: 1 [33280/50000 (66%)]\tLoss: 1.498368\n",
      "Train Epoch: 1 [33920/50000 (68%)]\tLoss: 1.568289\n",
      "Train Epoch: 1 [34560/50000 (69%)]\tLoss: 1.583278\n",
      "Train Epoch: 1 [35200/50000 (70%)]\tLoss: 1.472134\n",
      "Train Epoch: 1 [35840/50000 (72%)]\tLoss: 1.481882\n",
      "Train Epoch: 1 [36480/50000 (73%)]\tLoss: 1.420902\n",
      "Train Epoch: 1 [37120/50000 (74%)]\tLoss: 1.547705\n",
      "Train Epoch: 1 [37760/50000 (75%)]\tLoss: 1.367860\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 1.490786\n",
      "Train Epoch: 1 [39040/50000 (78%)]\tLoss: 1.496758\n",
      "Train Epoch: 1 [39680/50000 (79%)]\tLoss: 1.473996\n",
      "Train Epoch: 1 [40320/50000 (81%)]\tLoss: 1.206199\n",
      "Train Epoch: 1 [40960/50000 (82%)]\tLoss: 1.281926\n",
      "Train Epoch: 1 [41600/50000 (83%)]\tLoss: 1.509735\n",
      "Train Epoch: 1 [42240/50000 (84%)]\tLoss: 1.577026\n",
      "Train Epoch: 1 [42880/50000 (86%)]\tLoss: 1.286069\n",
      "Train Epoch: 1 [43520/50000 (87%)]\tLoss: 1.314901\n",
      "Train Epoch: 1 [44160/50000 (88%)]\tLoss: 1.366168\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 1.408926\n",
      "Train Epoch: 1 [45440/50000 (91%)]\tLoss: 1.405406\n",
      "Train Epoch: 1 [46080/50000 (92%)]\tLoss: 1.428885\n",
      "Train Epoch: 1 [46720/50000 (93%)]\tLoss: 1.394042\n",
      "Train Epoch: 1 [47360/50000 (95%)]\tLoss: 1.340829\n",
      "Train Epoch: 1 [48000/50000 (96%)]\tLoss: 1.503042\n",
      "Train Epoch: 1 [48640/50000 (97%)]\tLoss: 1.370558\n",
      "Train Epoch: 1 [49280/50000 (98%)]\tLoss: 1.367578\n",
      "Train Epoch: 1 [49920/50000 (100%)]\tLoss: 1.457453\n",
      "\n",
      "Test set: Average loss: 1.3542, Accuracy: 5158/10000 (52%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.249661\n",
      "Train Epoch: 2 [640/50000 (1%)]\tLoss: 1.393936\n",
      "Train Epoch: 2 [1280/50000 (3%)]\tLoss: 1.424540\n",
      "Train Epoch: 2 [1920/50000 (4%)]\tLoss: 1.575099\n",
      "Train Epoch: 2 [2560/50000 (5%)]\tLoss: 1.514821\n",
      "Train Epoch: 2 [3200/50000 (6%)]\tLoss: 1.697073\n",
      "Train Epoch: 2 [3840/50000 (8%)]\tLoss: 1.136999\n",
      "Train Epoch: 2 [4480/50000 (9%)]\tLoss: 1.273037\n",
      "Train Epoch: 2 [5120/50000 (10%)]\tLoss: 1.228996\n",
      "Train Epoch: 2 [5760/50000 (12%)]\tLoss: 1.145562\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 1.353050\n",
      "Train Epoch: 2 [7040/50000 (14%)]\tLoss: 1.511604\n",
      "Train Epoch: 2 [7680/50000 (15%)]\tLoss: 1.359294\n",
      "Train Epoch: 2 [8320/50000 (17%)]\tLoss: 1.212733\n",
      "Train Epoch: 2 [8960/50000 (18%)]\tLoss: 1.346549\n",
      "Train Epoch: 2 [9600/50000 (19%)]\tLoss: 1.597532\n",
      "Train Epoch: 2 [10240/50000 (20%)]\tLoss: 1.404942\n",
      "Train Epoch: 2 [10880/50000 (22%)]\tLoss: 1.486496\n",
      "Train Epoch: 2 [11520/50000 (23%)]\tLoss: 1.442061\n",
      "Train Epoch: 2 [12160/50000 (24%)]\tLoss: 1.446001\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 1.319425\n",
      "Train Epoch: 2 [13440/50000 (27%)]\tLoss: 1.256757\n",
      "Train Epoch: 2 [14080/50000 (28%)]\tLoss: 1.305001\n",
      "Train Epoch: 2 [14720/50000 (29%)]\tLoss: 1.357012\n",
      "Train Epoch: 2 [15360/50000 (31%)]\tLoss: 1.328203\n",
      "Train Epoch: 2 [16000/50000 (32%)]\tLoss: 1.164746\n",
      "Train Epoch: 2 [16640/50000 (33%)]\tLoss: 1.105455\n",
      "Train Epoch: 2 [17280/50000 (35%)]\tLoss: 1.135279\n",
      "Train Epoch: 2 [17920/50000 (36%)]\tLoss: 1.305868\n",
      "Train Epoch: 2 [18560/50000 (37%)]\tLoss: 1.163561\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 1.312583\n",
      "Train Epoch: 2 [19840/50000 (40%)]\tLoss: 1.225631\n",
      "Train Epoch: 2 [20480/50000 (41%)]\tLoss: 1.480438\n",
      "Train Epoch: 2 [21120/50000 (42%)]\tLoss: 1.196379\n",
      "Train Epoch: 2 [21760/50000 (43%)]\tLoss: 1.166639\n",
      "Train Epoch: 2 [22400/50000 (45%)]\tLoss: 1.388466\n",
      "Train Epoch: 2 [23040/50000 (46%)]\tLoss: 1.555185\n",
      "Train Epoch: 2 [23680/50000 (47%)]\tLoss: 1.240609\n",
      "Train Epoch: 2 [24320/50000 (49%)]\tLoss: 1.411466\n",
      "Train Epoch: 2 [24960/50000 (50%)]\tLoss: 1.300165\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 1.318691\n",
      "Train Epoch: 2 [26240/50000 (52%)]\tLoss: 1.365732\n",
      "Train Epoch: 2 [26880/50000 (54%)]\tLoss: 1.329565\n",
      "Train Epoch: 2 [27520/50000 (55%)]\tLoss: 1.531958\n",
      "Train Epoch: 2 [28160/50000 (56%)]\tLoss: 1.226514\n",
      "Train Epoch: 2 [28800/50000 (58%)]\tLoss: 1.319697\n",
      "Train Epoch: 2 [29440/50000 (59%)]\tLoss: 1.408654\n",
      "Train Epoch: 2 [30080/50000 (60%)]\tLoss: 1.345901\n",
      "Train Epoch: 2 [30720/50000 (61%)]\tLoss: 1.397620\n",
      "Train Epoch: 2 [31360/50000 (63%)]\tLoss: 1.390796\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 1.386968\n",
      "Train Epoch: 2 [32640/50000 (65%)]\tLoss: 1.346221\n",
      "Train Epoch: 2 [33280/50000 (66%)]\tLoss: 1.143060\n",
      "Train Epoch: 2 [33920/50000 (68%)]\tLoss: 1.109261\n",
      "Train Epoch: 2 [34560/50000 (69%)]\tLoss: 1.413994\n",
      "Train Epoch: 2 [35200/50000 (70%)]\tLoss: 1.374063\n",
      "Train Epoch: 2 [35840/50000 (72%)]\tLoss: 1.107002\n",
      "Train Epoch: 2 [36480/50000 (73%)]\tLoss: 1.405640\n",
      "Train Epoch: 2 [37120/50000 (74%)]\tLoss: 1.228821\n",
      "Train Epoch: 2 [37760/50000 (75%)]\tLoss: 1.439440\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 1.216972\n",
      "Train Epoch: 2 [39040/50000 (78%)]\tLoss: 1.394953\n",
      "Train Epoch: 2 [39680/50000 (79%)]\tLoss: 1.009281\n",
      "Train Epoch: 2 [40320/50000 (81%)]\tLoss: 1.460702\n",
      "Train Epoch: 2 [40960/50000 (82%)]\tLoss: 1.370158\n",
      "Train Epoch: 2 [41600/50000 (83%)]\tLoss: 1.183084\n",
      "Train Epoch: 2 [42240/50000 (84%)]\tLoss: 1.282032\n",
      "Train Epoch: 2 [42880/50000 (86%)]\tLoss: 1.314571\n",
      "Train Epoch: 2 [43520/50000 (87%)]\tLoss: 1.169862\n",
      "Train Epoch: 2 [44160/50000 (88%)]\tLoss: 1.253212\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 1.032452\n",
      "Train Epoch: 2 [45440/50000 (91%)]\tLoss: 1.211418\n",
      "Train Epoch: 2 [46080/50000 (92%)]\tLoss: 1.145948\n",
      "Train Epoch: 2 [46720/50000 (93%)]\tLoss: 1.494393\n",
      "Train Epoch: 2 [47360/50000 (95%)]\tLoss: 1.347089\n",
      "Train Epoch: 2 [48000/50000 (96%)]\tLoss: 1.261882\n",
      "Train Epoch: 2 [48640/50000 (97%)]\tLoss: 1.272124\n",
      "Train Epoch: 2 [49280/50000 (98%)]\tLoss: 1.077025\n",
      "Train Epoch: 2 [49920/50000 (100%)]\tLoss: 1.076764\n",
      "\n",
      "Test set: Average loss: 1.2668, Accuracy: 5515/10000 (55%)\n",
      "\n",
      "Files already downloaded and verified\n",
      "Evaluating MLP with CIFAR100\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 4.617598\n",
      "Train Epoch: 1 [640/50000 (1%)]\tLoss: 4.597711\n",
      "Train Epoch: 1 [1280/50000 (3%)]\tLoss: 4.618839\n",
      "Train Epoch: 1 [1920/50000 (4%)]\tLoss: 4.610132\n",
      "Train Epoch: 1 [2560/50000 (5%)]\tLoss: 4.613927\n",
      "Train Epoch: 1 [3200/50000 (6%)]\tLoss: 4.603985\n",
      "Train Epoch: 1 [3840/50000 (8%)]\tLoss: 4.632664\n",
      "Train Epoch: 1 [4480/50000 (9%)]\tLoss: 4.646700\n",
      "Train Epoch: 1 [5120/50000 (10%)]\tLoss: 4.603396\n",
      "Train Epoch: 1 [5760/50000 (12%)]\tLoss: 4.621096\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 4.605704\n",
      "Train Epoch: 1 [7040/50000 (14%)]\tLoss: 4.618133\n",
      "Train Epoch: 1 [7680/50000 (15%)]\tLoss: 4.610495\n",
      "Train Epoch: 1 [8320/50000 (17%)]\tLoss: 4.595138\n",
      "Train Epoch: 1 [8960/50000 (18%)]\tLoss: 4.610256\n",
      "Train Epoch: 1 [9600/50000 (19%)]\tLoss: 4.589515\n",
      "Train Epoch: 1 [10240/50000 (20%)]\tLoss: 4.591578\n",
      "Train Epoch: 1 [10880/50000 (22%)]\tLoss: 4.611112\n",
      "Train Epoch: 1 [11520/50000 (23%)]\tLoss: 4.598160\n",
      "Train Epoch: 1 [12160/50000 (24%)]\tLoss: 4.592697\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 4.597082\n",
      "Train Epoch: 1 [13440/50000 (27%)]\tLoss: 4.622268\n",
      "Train Epoch: 1 [14080/50000 (28%)]\tLoss: 4.601892\n",
      "Train Epoch: 1 [14720/50000 (29%)]\tLoss: 4.597668\n",
      "Train Epoch: 1 [15360/50000 (31%)]\tLoss: 4.587053\n",
      "Train Epoch: 1 [16000/50000 (32%)]\tLoss: 4.604107\n",
      "Train Epoch: 1 [16640/50000 (33%)]\tLoss: 4.600913\n",
      "Train Epoch: 1 [17280/50000 (35%)]\tLoss: 4.606600\n",
      "Train Epoch: 1 [17920/50000 (36%)]\tLoss: 4.581464\n",
      "Train Epoch: 1 [18560/50000 (37%)]\tLoss: 4.572056\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 4.605963\n",
      "Train Epoch: 1 [19840/50000 (40%)]\tLoss: 4.584872\n",
      "Train Epoch: 1 [20480/50000 (41%)]\tLoss: 4.594001\n",
      "Train Epoch: 1 [21120/50000 (42%)]\tLoss: 4.604238\n",
      "Train Epoch: 1 [21760/50000 (43%)]\tLoss: 4.606021\n",
      "Train Epoch: 1 [22400/50000 (45%)]\tLoss: 4.597575\n",
      "Train Epoch: 1 [23040/50000 (46%)]\tLoss: 4.602392\n",
      "Train Epoch: 1 [23680/50000 (47%)]\tLoss: 4.576120\n",
      "Train Epoch: 1 [24320/50000 (49%)]\tLoss: 4.566082\n",
      "Train Epoch: 1 [24960/50000 (50%)]\tLoss: 4.595481\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 4.564407\n",
      "Train Epoch: 1 [26240/50000 (52%)]\tLoss: 4.558185\n",
      "Train Epoch: 1 [26880/50000 (54%)]\tLoss: 4.598059\n",
      "Train Epoch: 1 [27520/50000 (55%)]\tLoss: 4.571430\n",
      "Train Epoch: 1 [28160/50000 (56%)]\tLoss: 4.579669\n",
      "Train Epoch: 1 [28800/50000 (58%)]\tLoss: 4.588394\n",
      "Train Epoch: 1 [29440/50000 (59%)]\tLoss: 4.578613\n",
      "Train Epoch: 1 [30080/50000 (60%)]\tLoss: 4.568532\n",
      "Train Epoch: 1 [30720/50000 (61%)]\tLoss: 4.580202\n",
      "Train Epoch: 1 [31360/50000 (63%)]\tLoss: 4.582043\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 4.592309\n",
      "Train Epoch: 1 [32640/50000 (65%)]\tLoss: 4.591810\n",
      "Train Epoch: 1 [33280/50000 (66%)]\tLoss: 4.595957\n",
      "Train Epoch: 1 [33920/50000 (68%)]\tLoss: 4.580693\n",
      "Train Epoch: 1 [34560/50000 (69%)]\tLoss: 4.596760\n",
      "Train Epoch: 1 [35200/50000 (70%)]\tLoss: 4.595530\n",
      "Train Epoch: 1 [35840/50000 (72%)]\tLoss: 4.573189\n",
      "Train Epoch: 1 [36480/50000 (73%)]\tLoss: 4.585809\n",
      "Train Epoch: 1 [37120/50000 (74%)]\tLoss: 4.596745\n",
      "Train Epoch: 1 [37760/50000 (75%)]\tLoss: 4.568358\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 4.568581\n",
      "Train Epoch: 1 [39040/50000 (78%)]\tLoss: 4.568258\n",
      "Train Epoch: 1 [39680/50000 (79%)]\tLoss: 4.568244\n",
      "Train Epoch: 1 [40320/50000 (81%)]\tLoss: 4.597220\n",
      "Train Epoch: 1 [40960/50000 (82%)]\tLoss: 4.591376\n",
      "Train Epoch: 1 [41600/50000 (83%)]\tLoss: 4.562422\n",
      "Train Epoch: 1 [42240/50000 (84%)]\tLoss: 4.581903\n",
      "Train Epoch: 1 [42880/50000 (86%)]\tLoss: 4.587336\n",
      "Train Epoch: 1 [43520/50000 (87%)]\tLoss: 4.559316\n",
      "Train Epoch: 1 [44160/50000 (88%)]\tLoss: 4.591531\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 4.555830\n",
      "Train Epoch: 1 [45440/50000 (91%)]\tLoss: 4.571259\n",
      "Train Epoch: 1 [46080/50000 (92%)]\tLoss: 4.534218\n",
      "Train Epoch: 1 [46720/50000 (93%)]\tLoss: 4.571434\n",
      "Train Epoch: 1 [47360/50000 (95%)]\tLoss: 4.578177\n",
      "Train Epoch: 1 [48000/50000 (96%)]\tLoss: 4.540414\n",
      "Train Epoch: 1 [48640/50000 (97%)]\tLoss: 4.552961\n",
      "Train Epoch: 1 [49280/50000 (98%)]\tLoss: 4.534617\n",
      "Train Epoch: 1 [49920/50000 (100%)]\tLoss: 4.599714\n",
      "\n",
      "Test set: Average loss: 4.5448, Accuracy: 285/10000 (3%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#DATASETS    = [\"MNIST\"] \n",
    "#MODEL_TYPES = [\"CNN\"]\n",
    "DATASETS    = [\"MNIST\", \"CIFAR10\", \"CIFAR100\"] \n",
    "MODEL_TYPES = [\"MLP\", \"CNN\", \"ViT\", \"HYBRID\"]\n",
    "\n",
    "batch_size      = 64             # Input batch size for training (default: 64)\n",
    "epochs          = 1             # Number of epochs to train (default: 15)\n",
    "lr              = .00001         # Learning rate (default: .00001)\n",
    "gamma           = 0.7            # Learning rate step gamma (default: 0.7)\n",
    "test_batch_size = 1000           # Input batch size for testing (default: 1000)\n",
    "log_dir         = \"./logs/tb\"\n",
    "log_interval    = 1000           # How many batches to wait before logging training status (default: 10)\n",
    "\n",
    "accuracies, parameters = run(batch_size, test_batch_size, lr, gamma, epochs, log_dir, log_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "times = Timer().get_phases()\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, sharex=True, figsize=(20,10))\n",
    "bar_colors = ['tab:red', 'tab:green', 'tab:blue', 'tab:orange']\n",
    "ax1.bar(list(times.keys()), list(times.values()), color=bar_colors)\n",
    "ax1.set_ylabel('Inference Times (ms)')\n",
    "ax2.bar(list(times.keys()), list(parameters.values()), color=bar_colors)\n",
    "ax2.set_ylabel('Total Parameters')\n",
    "ax3.bar(list(times.keys()), list(accuracies.values()), color=bar_colors)\n",
    "ax3.set_ylabel('Model Accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ik",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
